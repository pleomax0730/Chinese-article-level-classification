{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a535720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import process_dataset\n",
    "from argparse import ArgumentParser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fada8fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0f8587004575917f\n",
      "Reusing dataset csv (/home/vincent0730/.cache/huggingface/datasets/csv/default-0f8587004575917f/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3080465d67c5487d88e1843c70f298aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"/home/vincent0730/ML_pondlet_level_predictor/datasets/pondlet_STB_pondlet_20220803_content_data_train.csv\",\n",
    "        \"test\": \"/home/vincent0730/ML_pondlet_level_predictor/datasets/pondlet_STB_pondlet_20220803_content_data_test.csv\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "286c4202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'Length', 'content'],\n",
       "        num_rows: 1561\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'Length', 'content'],\n",
       "        num_rows: 174\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a887d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_labels at 0x7f6159d6e8b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Loading cached processed dataset at /home/vincent0730/.cache/huggingface/datasets/csv/default-0f8587004575917f/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-44867db30d67b366.arrow\n",
      "Loading cached processed dataset at /home/vincent0730/.cache/huggingface/datasets/csv/default-0f8587004575917f/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-c4da537c1651ddae.arrow\n"
     ]
    }
   ],
   "source": [
    "label_list = dataset[\"train\"].unique(\"labels\")\n",
    "label_list.sort()\n",
    "num_labels = len(label_list)\n",
    "label_to_id = {}\n",
    "id_to_label = {}\n",
    "for i, label in enumerate(label_list):\n",
    "    label_to_id[label] = i\n",
    "    id_to_label[i] = label\n",
    "\n",
    "\n",
    "def preprocess_labels(examples):\n",
    "    if label_to_id is not None and \"labels\" in examples:\n",
    "        examples[\"labels\"] = [label_to_id[l] for l in examples[\"labels\"]]\n",
    "    return examples\n",
    "\n",
    "\n",
    "dataset = dataset.map(preprocess_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc8a701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lv.0': 0, 'Lv.1': 1, 'Lv.2': 2, 'Lv.3': 3, 'Lv.4': 4, 'Lv.5': 5}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8098fdb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': 5,\n",
       " 'Length': 519,\n",
       " 'content': '如今一个企业拼的是人才，一件产品拼的 是技术含量，一项文化活动拼的是创意。 有了好的创意，就是白菜豆腐到了烹饪大 师手中，也能做出一盘大菜来苏州人真的当起了“烹饪大师”。那里的金 鸡湖，木渎古镇、山塘街等地前几年我也 去过，这些地方好是好，但总比不上虎 丘、拙政园、水乡周庄来得显赫。现在可 不同了，金鸡湖能媲美“西湖”，木渎偏以 园林见长，山塘街的游客也成倍地猛增， 靠的是什么？还不是人家巧打了创意牌， 才赢得了游客的认同与赞赏创新！关键在创新！这是一个民族进步的 灵魂，是各行各业兴旺不竭的动力，也是 旅游业永葆生机的“源头活水”。只有在不 断创新上下功夫，我国的旅游业才能保持 竞争力和可持续发展的能力可是看看某些地方的旅游开发，确实让人 揪心。机械性地照搬照抄，或掠夺性地过 度开发，使大量的旅游景点停留在低水 平、单调重复、小散全的状态，有的只能 靠“门票经济”吃饭。就拿“西游记宫”来说 吧，有人统计全国造了 820 多个，这种低层次的简单模仿，其结果可想而知：只 能是一批又一批地倒闭。旅游创新是一道智慧题。必须突破传统思 维定式，敢于不按常规出牌，需要周密的 策划与高超的技巧。让我们一起打好“创 意牌”，擦亮创意火花，让旅游景点开出 一朵朵“领异标新二月花”，清香宜人。'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "570ddb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec4661ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import SetFitModel\n",
    "\n",
    "def make_model(params=None):\n",
    "    model_id = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    return SetFitModel.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de4c49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "\n",
    "from setfit import SetFitTrainer\n",
    "\n",
    "trainer = SetFitTrainer(\n",
    "    model_init=make_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    num_epochs=3,\n",
    "    num_iterations=20,\n",
    "    column_mapping={\"content\": \"text\", \"labels\": \"label\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "687bfdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search_function(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [24, 32]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f86ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-02 11:41:52,080]\u001b[0m A new study created in memory with name: no-name-69897ec6-aa1f-4613-8072-e96993b4b127\u001b[0m\n",
      "Trial: {'learning_rate': 0.00014915537758629986, 'batch_size': 24}\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "***** Running training *****\n",
      "  Num examples = 62440\n",
      "  Num epochs = 3\n",
      "  Total optimization steps = 2602\n",
      "  Total train batch size = 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2d95ac7dce4833a6f3d56ec1377cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376176f746624839a955202235c31187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best = trainer.hyperparameter_search(hyperparameter_search_function, n_trials=10)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7363a452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "61547a2d9de180a2f595997338d17f6007f31620c7ca1d675a5afe3da83bc92f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
